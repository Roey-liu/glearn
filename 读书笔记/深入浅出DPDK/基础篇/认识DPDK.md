
什么是DPDK 简单的可以理解为一个用户态的网卡驱动 
我们先了解下DPDK要解决什么问题？


传统的网络设备包处理方式
```
.数据包到达网卡设备。
·网卡设备依据配置进行DMA操作。
·网卡发送中断， 唤醒处理器。
·驱动软件填充读写缓冲区数据结构。
·数据报文达到内核协议栈，进行高层处理。
·如果最终应用在用户态，数据从内核搬移到用户态。
·如果最终应用在内核态，在内核继续进行。
```

这样有什么问题？  
随着网络接口带宽从千兆向万兆迈进， 原先每个报文就会触发一个中断， 中断带来的开销变得突出。


网络包进入计算机大多需要经过协议处理， 在Linux系统中TCP/IP
由Linux内核处理。 即使在不需要协议处理的场景下， 大多数场景下也需要把包从内核的缓冲区复制到用户缓冲区，系统调用以及数据包复制的开销， 会直接影响用户态应用从设备直接获得包的能力。 

有个著名的高性能网络I/O框架Netmap， 它就是采用共享数据包池的方式， 减少内核到
用户空间的包复制。

Netmap有什么缺点？
现今CPU核数越来越多，性能越来越强，为了追求极端的高性能高
效率， 分时就不一定总是上佳的策略。 以Netmap来说， 即便其减少了内核到用户空间的内存复制， 但内核驱动的收发包处理和用户态线程依旧
由操作系统调度执行， 除去任务切换本身的开销， 由切换导致的后续
cache替换（不同任务内存热点不同） ， 对性能也会产生负面的影响


来看看DPDK怎么搞的？

轮询， 这一点很直接， 可避免中断上下文切换的开销。

用户态驱动， 在这种工作方式下， 既规避了不必要的内存拷贝又避
免了系统调用。 


亲和性与独占， DPDK工作在用户态， 线程的调度仍然依赖内核。
利用线程的CPU亲和绑定的方式， 特定任务可以被指定只在某个核上工
作。


降低访存开销， 网络数据包处理是一种典型的I/O密集型（I/O
bound） 工作负载。 无论是CPU指令还是DMA， 对于内存子系统
（Cache+DRAM） 都会访问频繁。 利用一些已知的高效方法来减少访存
的开销能够有效地提升性能。 比如利用内存大页能有效降低TLB miss，
比如利用内存多通道的交错访问能有效提高内存访问的有效带宽， 再比
如利用对于内存非对称性的感知可以避免额外的访存延迟。 而cache更
是几乎所有优化的核心地带， 


利用IA新硬件技术。 如Intel® DDIO技术。 有效利
用SIMD（Single Instruction Multiple Data） 并结合超标量技术
（Superscalar） 对数据层面或者对指令层面进行深度并行化， 在性能的
进一步提升上也行之有效。


充分挖掘网卡的潜能， 经过DPDK I/O加速的数据包通过PCIe网卡
进入系统内存， PCIe外设到系统内存之间的带宽利用效率、 数据传送方
式（coalesce操作） 等都是直接影响I/O性能的因素。 在现代网卡中， 往
往还支持一些分流（如RSS， FDIR等） 和卸载（如Chksum， TSO等）
功能。 DPDK充分利用这些硬件加速特性， 帮助应用更好地获得直接的
性能提升。


DPDK框架简介

核心库Core Libs， 提供系统抽象、 大页内存、 缓存池、 定时器及无
锁环等基础组件。
PMD库， 提供全用户态的驱动， 以便通过轮询和线程绑定得到极高
的网络吞吐， 支持各种本地和虚拟的网卡。
Classify库， 支持精确匹配（Exact Match） 、 最长匹配（LPM） 和
通配符匹配（ACL） ， 提供常用包处理的查表操作。
QoS库， 提供网络服务质量相关组件， 如限速（Meter） 和调度
（Sched）