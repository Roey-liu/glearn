在传统的内核协议栈中，网络包处理存在诸多瓶颈，严重影响网络包的收发性能，

# DPDK简介


传统的网络设备包处理方式
```
.数据包到达网卡设备。
·网卡设备依据配置进行DMA操作。
·网卡发送中断， 唤醒处理器。
·驱动软件填充读写缓冲区数据结构。
·数据报文达到内核协议栈，进行高层处理。
·如果最终应用在用户态，数据从内核搬移到用户态。
·如果最终应用在内核态，在内核继续进行。
```


## 性能瓶颈主要包括以下几个方面 


<div align="center"> <img src="pic/data_kernel.png"/> </div> 

###  局部性失效 

一个数据包的处理可能跨多个CPU核心、缓存失效、NUMA不友好，一个数据包可能中断在cpu0，内核态处理在cpu1，用户态处理在cpu2，这样跨越多个核心，造成局部性失效，CPU缓存失效，同时可能存在跨NUMA访问内存，性能受到很大影响。

### 中断处理 - 硬件中断、软中断、上下文切换  

当网络中数据量很大时，大量的数据包产生频繁的硬件中断请求，这些硬件中断可以打断之前较低优先级的软中断或者系统调用的执行过程，如果这种打断频繁进行的话，将产生较高的性能开销。用户态内核态的上下文切换和软中断都增加了额外的开销。

### 内存拷贝 - 内核态和用户态之间的内存拷贝 

网络数据包从网卡到应用程序需要经过如下的过程： 数据从网卡通过DMA等方式传到内核开辟的缓冲区； 数据从内核空间复制到用户态空间。在Linux内核协议栈中，这个耗时甚至占到了数据包整个处理流程的一半。

###  系统调用 - 软中断、上下文切换、锁竞争 

频繁到达的硬件中断或者软中断都可能随时抢占系统调用的运行，这也将产生大量的上下文切换开销。内核中一些资源如PCB表等都需要加锁处理，大量的并发操作造成很大的性能浪费，特别是大量短连接的创建。

## 传统Linux网络驱动的问题

* 中断开销突出，大量数据到来会触发频繁的中断（softirq）开销导致系统无法承受
* 需要把包从内核缓冲区拷贝到用户缓冲区，带来系统调用和数据包复制的开销
* 对于很多网络功能节点来说，TCP/IP协议并非是数据转发环节所必需的
* NAPI/Netmap等虽然减少了内核到用户空间的数据拷贝，但操作系统调度带来的cache替换也会对性能产生负面影响



## DPDK最佳实践

### PMD用户态驱动: 
DPDK针对Intel网卡实现了基于轮询方式的PMD（Poll Mode Drivers）驱动，该驱动由API、用户空间运行的驱动程序构成，该驱动使用无中断方式直接操作网卡的接收和发送队列（除了链路状态通知仍必须采用中断方式以外）。

PMD驱动从网卡上接收到数据包后，会直接通过DMA方式传输到预分配的内存中，同时更新无锁环形队列中的数据包指针，不断轮询的应用程序很快就能感知收到数据包，并在预分配的内存地址上直接处理数据包，这个过程非常简洁。


### hugetlbfs: 
 这样有两个好处：
 * 第一是使用hugepage的内存所需的页表项比较少，对于需要大量内存的进程来说节省了很多开销，像oracle之类的大型数据库优化都使用了大页面配置；、
 * 第二是TLB冲突概率降低，TLB是cpu中单独的一块高速cache，采用hugepage可以大大降低TLB miss的开销

### CPU亲缘性和独占: 

多核则是每个CPU核一个线程，核心之间访问数据无需上锁。为了最大限度减少线程调度的资源消耗，需要将Linux绑定在特定的核上，释放其余核心来专供应用程序使用。 同时还需要考虑CPU特性和系统是否支持NUMA架构，如果支持的话，不同插槽上CPU的进程要避免访问远端内存，尽量访问本端内存。
* 避免不同核之间的频繁切换，从而避免cache miss和cache write back
* 避免同一个核内多任务切换开销

### 降低内存访问开销
* 借助大页降低TLB miss
* 利用内存多通道交错访问提高内存访问的有效带宽
* 利用内存非对称性感知避免额外的访存延迟
* 少用数组和指针，多用局部变量
* 少用全局变量
* 一次多访问一些数据
* 自己管理内存分配；进程间传递指针而非整个数据块

### 避免False Sharing（伪共享）: 

false sharing（伪共享）问题
* 多核CPU中每个核都拥有自己的L1/L2 cache，当运行多线程程序时，尽管算法上不需要共享变量，但实际执行中两个线程访问同一cache line的数据时就会引起冲突，每个线程在读取自己的数据时也会把别人的cacheline读进来，这时一个核修改改变量，CPU的cache一致性算法会迫使另一个核的cache中包含该变量所在的cache line无效，这就产生了false sharing（伪共享）问题

访问全局变量和动态分配内存是falsesharing问题产生的根源，当然访问在内存中相邻的但完全不同的全局变量也可能会导致false sharing，
* **多使用线程本地变量是解决false sharing的根源办法**。

### 内存对齐：
根据不同存储硬件的配置来优化程序，性能也能够得到极大的提升。

字节对齐：
* 众所周知，内存最小的存储单元为字节，在32位CPU中，寄存器也是32位的，为了保证访问更加高效，在32位系统中变量存储的起始地址默认是4的倍数（64位系统则是8的倍数）


### cache对齐

Cache line是CPU从内存加载数据的最小单位，一般L1 cache的cache line大小为64字节。如果CPU访问的变量不在cache中，就需要先从内存调入到cache，调度的最小单位就是cache line。因此，内存访问如果没有按照cache line边界对齐，就会多读写一次内存和cache了。

### 减少进程上下文切换:

使用线程池模型：让每个线程工作前都持有带计数器的信号量，在信号量达到最大值之前，每个线程被唤醒时仅进行一次上下文切换，当信号量达到最大值时，其它线程都不会再竞争资源了

### 充分挖掘网卡的潜能

借助现代网卡支持的分流（RSS, FDIR）和卸载（TSO，chksum）等特性。


### 利用IA新硬件技术。 
如Intel® DDIO技术。有效利用SIMD（Single Instruction Multiple Data） 并结合超标量技术（Superscalar） 对数据层面或者对指令层面进行深度并行化， 在性能的进一步提升上也行之有效。

---

## Cache子系统

* 一级Cache：4个指令周期，分为数据cache和指令cache，一般只有几十KB
* 二级Cache：12个指令周期，几百KB到几MB
* 三级Cache：26-31个指令周期，几MB到几十MB
* TLB Cache：缓存内存中的页表项，减少CPU开销

如何把内存中的内容放到cache中呢？这里需要映射算法和分块机制。当今主流块大小是64字节。

硬件Cache预取（Netburst为例）：
* 只有两次cache miss才能激活预取机制，且2次的内存地址偏差不超过256或512字节
* 一个4KB的page内只定义一条stream
* 能同时独立的追踪8条stream
* 对4KB边界之外不进行预取
* 预取的数据放在二级或三级cache中
* 对strong uncacheable和write combining内存类型不预取

硬件预取不一定能够提升性能，所以DPDK还借助软件预取尽量将数据放到cache中。另外，DPDK在定义数据结构的时候还保证了cache line对齐


cache一致性

* 原则是避免多个核访问同一个内存地址或数据结构
* 在数据结构上：每个核都有独立的数据结构
* 多个核访问同一个网卡：每个核都创建单独的接收队列和发送队列

## Data Direct I/O (DDIO)
DDIO使得外部网卡和CPU通过LLC cache直接交换数据，绕过了内存，增加了CPU处理报文的速度。

在Intel E5系列产品中，LLC Cache的容量提高到了20MB。


<div align="center"> <img src="pic/ddio.png"/> </div> 


## NUMA
NUMA来源于AMD Opteron微架构，处理器和本地内存之间有更小的延迟和更大的带宽；每个处理器还可以有自己的总线。处理器访问本地的总线和内存时延迟低，而访问远程资源时则要高

<div align="center"> <img src="pic/numa.png"/> </div> 

DPDK充分利用了NUMA的特点

* Per-core memory，每个核都有自己的内存，一方面是本地内存的需要，另一方面也是为了cache一致性
* 用本地处理器和本地内存处理本地设备上产生的数据

```cpp
q = rte_zmalloc_socket("fm10k", sizeof(*q), RTE_CACHE_LINE_SIZE, socket_id)
```

## CPU亲和性

将进程与CPU绑定，提高了Cache命中率，从而减少内存访问损耗。CPU亲和性的主要应用场景为

* 大量计算场景
* 运行时间敏感、决定性的线程，即实时线程

相关工具

* sched_set_affinity()、sched_get_affinity()内核函数
* taskset命令
* isolcpus内核启动参数：CPU绑定之后依然是有可能发生线程切换，可以借助isolcpus=2,3将cpu从内核调度系统中剥离。

DPDK中的CPU亲和性

* DPDK中lcore实际上是EAL pthread，每个EAL pthread都有一个Thread Local Storage的_lcore_id，_lcore_id与CPU ID是一致的。

## 指令并发

SIMD（Single Instruction Multiple Data，单指令多数据）
* 可以最大化的利用一级缓存访存的带宽，但对频繁的窄位宽数据操作就有比较大的副作用。

DPDK中的rte_memcpy()在Intel处理器上充分利用了SSE/AVX的特点：
* 优先保证Store指令存储的地址对齐，然后在每个指令周期指令2条Load的特新弥补一部分非对齐Load带来的性能损失。