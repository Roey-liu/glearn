
##  简单介绍

参考文章：[链接](https://www.cnblogs.com/qcloud1001/p/9585724.html)

简单来说,在接下来的 10 年里，因为 IPv6 协议下每个服务器的潜在连接数都是数以百万 级的，单机服务器处理数百万的并发连接（甚至千万）并非不可能，但我们需要重新审视目 前主流 OS 针对网络编程这一块的具体技术实现。 


## 解决 C10M 问题关键所在

这个观点是一个大佬提出的 Robert Graham 的结论是：
> OS 的内核不是解决 C10M 问题的办法，恰恰相反 OS 的内核正是 导致 C10M 问题的关键所在。 

这个结论意味着啥？
> 不要让 OS 内核执行所有繁重的任务：将数据包处理、内存管理、处理器调度等任务从内核 转移到应用程序高效地完成，让诸如 Linux 这样的 OS 只处理控制层，数据层完全交给应用 程序来处理

## 为什么说实现 C10M 的挑战不在硬件而在软件？ 
```
硬件不是 10M 问题的性能瓶颈所在处，真正的问题出在软件上，尤其是Linux 操作系统 

理由如下面这几点： 

首先：最初的设计是让 Unix 成为一个电话网络的控制系统，而不是成为一个服务器操 作系统。对于控制系统而言，针对的主要目标是用户和任务，而并没有针对作为协助功能的 数据处理做特别设计，也就是既没有所谓的快速路径、慢速路径，也没有各种数据服务处理 的优先级差别。 

其次：传统的 CPU，因为只有一个核，操作系统代码以多线程或多任务的形式来提升整 体性能。而现在，4 核、8 核、32 核、64 核和 100 核，都已经是真实存在的 CPU 芯片，如何提高多核的性能可扩展性，是一个必须面对的问题。比如让同一任务分割在多个核心上执行，以避免 CPU 的空闲浪费，当然，这里面要解决的技术点有任务分割、任务同步和异步 等。 

再次：核心缓存大小与内存速度是一个关键问题。现在，内存已经变得非常的便宜，随 便一台普通的笔记本电脑，内存至少也就是 4G 以上，高端服务器的内存上 24G 那是相当的 平常。但是，内存的访问速度仍然很慢，CPU 访问一次内存需要约 60~100 纳秒，相比很久 以前的内存访问速度，这基本没有增长多少。 

```

## 实现 C10M 意味着什么？
```
1 千万的并发连接数；   
100 万个连接/秒：每个连接以这个速率持续约 10 秒；  
10GB/秒的连接：快速连接到互联网；  
1 千万个数据包/秒：据估计目前的服务器每秒处理 50K 数据包，以后会更多；  
10 微秒的延迟：可扩展服务器也许可以处理这个规模（但延迟可能会飙升）；  
10 微秒的抖动：限制最大延迟；   
并发 10 核技术：软件应支持更多核的服务器（通常情况下，软件能轻松扩展到四核， 服务器可以扩展到更多核，因此需要重写软件，以支持更多核的服务器）。
``` 
##  关于Intel的DPDK 框架/ Netmap 开源框架
```
随着网络技术的不断创新和市场的发展，越来越多的网络设备基础架构开始向基于通用处理 器平台的架构方向融合，期望用更低的成本和更短的产品开发周期来提供多样的网络单元和 丰富的功能，如应用处理、控制处理、包处理、信号处理等。为了适应这一新的产业趋势，
 Intel 推出了基于 Intel x86 架构 DPDK (Data Plane Development Kit，数据平面开发套件) 实现 了高效灵活的包处理解决方案。经过近 6 年的发展，DPDK 已经发展成支持多种高性能网卡 和多通用处理器平台的开源软件工具包。 
```

## 解决C10M 问题的思路总结 

#### 网卡问题 
网卡问题：通过内核工作效率不高   
解决方案：使用自己的驱动程序并管理它们，使适配器远离操作系统。 
#### CPU 问题 
CPU 问题：使用传统的内核方法来协调你的应用程序是行不通的。   
解决方案：Linux 管理前两个 CPU，你的应用程序管理其余的 CPU，中断只发生在你允许的 CPU 上。 
#### 内存问题 
内存问题：内存需要特别关注，以求高效。  
解决方案：在系统启动时就分配大部分内存给你管理的大内存页。 