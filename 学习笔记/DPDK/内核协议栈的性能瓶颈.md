在传统的内核协议栈中，网络包处理存在诸多瓶颈，严重影响网络包的收发性能，



传统的网络设备包处理方式
```
.数据包到达网卡设备。
·网卡设备依据配置进行DMA操作。
·网卡发送中断， 唤醒处理器。
·驱动软件填充读写缓冲区数据结构。
·数据报文达到内核协议栈，进行高层处理。
·如果最终应用在用户态，数据从内核搬移到用户态。
·如果最终应用在内核态，在内核继续进行。
```


## 性能瓶颈主要包括以下几个方面 


<div align="center"> <img src="pic/data_kernel.png"/> </div> 

###  局部性失效 

一个数据包的处理可能跨多个CPU核心、缓存失效、NUMA不友好，一个数据包可能中断在cpu0，内核态处理在cpu1，用户态处理在cpu2，这样跨越多个核心，造成局部性失效，CPU缓存失效，同时可能存在跨NUMA访问内存，性能受到很大影响。

### 中断处理 - 硬件中断、软中断、上下文切换  

当网络中数据量很大时，大量的数据包产生频繁的硬件中断请求，这些硬件中断可以打断之前较低优先级的软中断或者系统调用的执行过程，如果这种打断频繁进行的话，将产生较高的性能开销。用户态内核态的上下文切换和软中断都增加了额外的开销。

### 内存拷贝 - 内核态和用户态之间的内存拷贝 

网络数据包从网卡到应用程序需要经过如下的过程： 数据从网卡通过DMA等方式传到内核开辟的缓冲区； 数据从内核空间复制到用户态空间。在Linux内核协议栈中，这个耗时甚至占到了数据包整个处理流程的一半。

###  系统调用 - 软中断、上下文切换、锁竞争 

频繁到达的硬件中断或者软中断都可能随时抢占系统调用的运行，这也将产生大量的上下文切换开销。内核中一些资源如PCB表等都需要加锁处理，大量的并发操作造成很大的性能浪费，特别是大量短连接的创建。


## 来看看DPDK怎么搞的？

### 轮询， 
这一点很直接， 可避免中断上下文切换的开销。


### 用户态驱动， 

在这种工作方式下， 既规避了不必要的内存拷贝又避
免了系统调用。 


### 亲和性与独占， 
DPDK工作在用户态， 线程的调度仍然依赖内核。
利用线程的CPU亲和绑定的方式， 特定任务可以被指定只在某个核上工
作。


### 降低访存开销， 
网络数据包处理是一种典型的I/O密集型（I/O bound） 工作负载。 无论是CPU指令还是DMA， 对于内存子系统
（Cache+DRAM） 都会访问频繁。 利用一些已知的高效方法来减少访存
的开销能够有效地提升性能。 比如利用内存大页能有效降低TLB miss，
比如利用内存多通道的交错访问能有效提高内存访问的有效带宽， 再比
如利用对于内存非对称性的感知可以避免额外的访存延迟。 而cache更
是几乎所有优化的核心地带， 


### 利用IA新硬件技术。 
如Intel® DDIO技术。有效利用SIMD（Single Instruction Multiple Data） 并结合超标量技术（Superscalar） 对数据层面或者对指令层面进行深度并行化， 在性能的进一步提升上也行之有效。


#### 充分挖掘网卡的潜能， 
经过DPDK I/O加速的数据包通过PCIe网卡进入系统内存， PCIe外设到系统内存之间的带宽利用效率、 数据传送方式（coalesce操作） 等都是直接影响I/O性能的因素。 在现代网卡中， 往往还支持一些分流（如RSS， FDIR等） 和卸载（如Chksum， TSO等）功能。 DPDK充分利用这些硬件加速特性， 帮助应用更好地获得直接的性能提升。